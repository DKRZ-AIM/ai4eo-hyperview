{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Section of the book chapter: <b>5.1 Supervised Learning Models</b>\n",
    "</div>\n",
    "\n",
    "# 3. Supervised learning\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "* [3.1 Linear regression and partial least squares](#3.1-Linear-regression-and-partial-least-squares)\n",
    "* [3.2 Tree-based Models](#3.2-Tree-based-Models)\n",
    "* [3.3 Support Vector Machines](#3.3-Support-Vector-Machines)\n",
    "* [3.4 k-Nearest Neighbors](#3.4-k-Nearest-Neighbors)\n",
    "* [3.5 Artificial Neural Networks (ANN)](#3.5-Artificial-Neural-Networks,-ANN)\n",
    "* [3.6 SUSI: Supervised Self-organizing Maps in Python](#3.6-SUSI:-Supervised-Self-organizing-Maps-in-Python)\n",
    "* [3.7 Overall results](#3.7-Overall-results)\n",
    "\n",
    "**Learnings:**\n",
    "\n",
    "- how to implement different supervised machine learning models,\n",
    "- how to plot regression results.\n",
    "\n",
    "\n",
    "\n",
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import sklearn.metrics as met\n",
    "import datetime\n",
    "\n",
    "import utils\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data\n",
    "\n",
    "**Dataset:** Felix M. Riese and Sina Keller, \"Hyperspectral benchmark dataset on soil moisture\", Dataset, Zenodo, 2018. [DOI:10.5281/zenodo.1227836](http://doi.org/10.5281/zenodo.1227836) and [GitHub](https://github.com/felixriese/hyperspectral-soilmoisture-dataset)\n",
    "\n",
    "**Introducing paper:** Felix M. Riese and Sina Keller, “Introducing a Framework of Self-Organizing Maps for Regression of Soil Moisture with Hyperspectral Data,” in IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium, Valencia, Spain, 2018, pp. 6151-6154. [DOI:10.1109/IGARSS.2018.8517812](https://doi.org/10.1109/IGARSS.2018.8517812)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 268, 284) (50, 268, 284) (50,) (50,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = utils.get_xy_split()\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = mpl.colors.Normalize(vmin=np.min([np.min(y_train), np.min(y_test)]),\n",
    "                            vmax=np.max([np.max(y_train), np.max(y_test)]))\n",
    "cmap = \"cividis_r\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=[\"model\", \"r2\", \"mae\", \"rmse\", \"potential\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "The following functions calculate and print the following performance metrics:\n",
    "\n",
    "* Coefficient of Determination $R^2$\n",
    "* Mean Absolute Error (MEA)\n",
    "* Root Mean Squared Error (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_metrics(y_pred):\n",
    "    global y_test\n",
    "    return (\n",
    "        met.r2_score(y_test, y_pred),\n",
    "        met.mean_absolute_error(y_test, y_pred),\n",
    "        np.sqrt(met.mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "def print_regression_metrics(y_pred, model_name, potential):\n",
    "    global results\n",
    "    \n",
    "    # get and print metrics\n",
    "    r2, mae, rmse = get_regression_metrics(y_pred)\n",
    "    print(\"R2 =   {0:.1f}% \\nMAE =  {1:.2f} \\nRMSE = {2:.2f}\".format(\n",
    "        r2*100, mae, rmse))\n",
    "    \n",
    "    # save metrics to dataframe\n",
    "    if not ((results[\"model\"]==model_name).any()):\n",
    "        rdict = {\n",
    "            \"model\": model_name,\n",
    "            \"r2\": r2,\n",
    "            \"mae\": mae,\n",
    "            \"rmse\": rmse,\n",
    "            \"potential\": potential}\n",
    "        results = results.append(rdict, ignore_index=True)\n",
    "        \n",
    "    else:\n",
    "        idx = results.index[results['model'] == model_name].tolist()[0]\n",
    "        results.at[idx, \"r2\"] = r2\n",
    "        results.at[idx, \"mae\"] = mae\n",
    "        results.at[idx, \"rmse\"] = rmse\n",
    "        results.at[idx, \"potential\"] = potential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Random State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 15:56:39.011208: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-30 15:56:39.011226: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 3.1 Linear regression and partial least squares\n",
    "\n",
    "Content:\n",
    "\n",
    "- [3.1.1 Linear regression](#3.1.1-Linear-regression)\n",
    "- [3.1.2 Partial least squares](#3.1.2-Partial-least-squares)\n",
    "\n",
    "### 3.1.1 Linear regression\n",
    "Implementation: [sklearn.linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. Estimator expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/notebooks/3_Supervised_Learning.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/notebooks/3_Supervised_Learning.ipynb#ch0000013?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m LinearRegression\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/notebooks/3_Supervised_Learning.ipynb#ch0000013?line=2'>3</a>\u001b[0m model_lin \u001b[39m=\u001b[39m LinearRegression()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/notebooks/3_Supervised_Learning.ipynb#ch0000013?line=3'>4</a>\u001b[0m model_lin\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/notebooks/3_Supervised_Learning.ipynb#ch0000013?line=4'>5</a>\u001b[0m y_pred_lin \u001b[39m=\u001b[39m model_lin\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/notebooks/3_Supervised_Learning.ipynb#ch0000013?line=6'>7</a>\u001b[0m print_regression_metrics(y_pred_lin, \u001b[39m\"\u001b[39m\u001b[39mLinear\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/linear_model/_base.py:662\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/linear_model/_base.py?line=657'>658</a>\u001b[0m n_jobs_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/linear_model/_base.py?line=659'>660</a>\u001b[0m accept_sparse \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositive \u001b[39melse\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcoo\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m--> <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/linear_model/_base.py?line=661'>662</a>\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/linear_model/_base.py?line=662'>663</a>\u001b[0m     X, y, accept_sparse\u001b[39m=\u001b[39;49maccept_sparse, y_numeric\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/linear_model/_base.py?line=663'>664</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/linear_model/_base.py?line=665'>666</a>\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/linear_model/_base.py?line=666'>667</a>\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/base.py?line=578'>579</a>\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/base.py?line=579'>580</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/base.py?line=580'>581</a>\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/base.py?line=581'>582</a>\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/base.py?line=583'>584</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py:964\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py?line=960'>961</a>\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py?line=961'>962</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39my cannot be None\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py?line=963'>964</a>\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py?line=964'>965</a>\u001b[0m     X,\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py?line=965'>966</a>\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py?line=966'>967</a>\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py?line=967'>968</a>\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py?line=968'>969</a>\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py?line=969'>970</a>\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py?line=970'>971</a>\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py?line=971'>972</a>\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py?line=972'>973</a>\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py?line=973'>974</a>\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py?line=974'>975</a>\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py?line=975'>976</a>\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py?line=976'>977</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py?line=978'>979</a>\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric)\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py?line=980'>981</a>\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py:794\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py?line=788'>789</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py?line=789'>790</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnable to convert array of bytes/strings \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py?line=790'>791</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39minto decimal numbers with dtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py?line=791'>792</a>\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py?line=792'>793</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nd \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[0;32m--> <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py?line=793'>794</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py?line=794'>795</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py?line=795'>796</a>\u001b[0m         \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py?line=796'>797</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py?line=798'>799</a>\u001b[0m \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m    <a href='file:///home/kone_ka/dev/helmholtz_hackathon/hyperspectral-regression/venv/lib/python3.8/site-packages/sklearn/utils/validation.py?line=799'>800</a>\u001b[0m     _assert_all_finite(array, allow_nan\u001b[39m=\u001b[39mforce_all_finite \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. Estimator expected <= 2."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model_lin = LinearRegression()\n",
    "model_lin.fit(X_train, y_train)\n",
    "y_pred_lin = model_lin.predict(X_test)\n",
    "\n",
    "print_regression_metrics(y_pred_lin, \"Linear\", \"-\")\n",
    "utils.plot_regression_results(y_test, y_pred_lin, \"Linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Partial least squares\n",
    "Implementation: [sklearn.cross_decomposition.PLSRegression](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "model_pls = PLSRegression(n_components=5)\n",
    "model_pls.fit(X_train, y_train)\n",
    "y_pred_pls = model_pls.predict(X_test)\n",
    "\n",
    "print_regression_metrics(y_pred_pls, \"PLS\", \"Minor\")\n",
    "utils.plot_regression_results(y_test, y_pred_pls, \"PLS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 3.2 Tree-based Models\n",
    "\n",
    "Content:\n",
    "\n",
    "- [3.2.1 Decision Tree](#3.2.1-Decision-Tree)\n",
    "- [3.2.2 Bagging: Random Forest & Extremly Randomized Trees](#3.2.2-Bagging:-Random-Forest-&-Extremly-Randomized-Trees)\n",
    "- [3.2.3 Boosting: Gradient Boosting](#3.2.3-Boosting:-Gradient-Boosting)\n",
    "\n",
    "### 3.2.1 Decision Tree\n",
    "\n",
    "**Source:** Breiman, L., Friedman, J., Olshen, R.A., Stone, C.J.: Classification and regression trees. Chapman and Hall/CRC (1984)\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "The regression trees algorithm is defined as follows:\n",
    "1. Start with the root node.\n",
    "2. Start with the most significant feature of the training data.\n",
    "3. Divide the input data with (binary) a cut $c_1$ on feature $x_i$, e.g. according to the Gini index, see below.\n",
    "4. Divide data along the next best feature on cut $c_j$ for $j=2, 3, \\ldots$\n",
    "5. Stop if a condition is met, e.g. maximum number of nodes, maximum depth, maximum purity etc.\n",
    "6. Every leaf is then averaged and therefore contains one output value.\n",
    "\n",
    "The Gini index is defined as:\n",
    "\n",
    "$G = 1 - \\sum_{i=1}^n P_i^2 \\qquad \\text{with } P_i = \\frac{N_i}{N},\\label{eq:gini}$\n",
    "\n",
    "with $N$ as number of all objects and $N_i$ as number of objects of class $i$.\n",
    "\n",
    "**Implementation:** [sklearn.tree.DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "model_dt = DecisionTreeRegressor()\n",
    "model_dt.fit(X_train, y_train)\n",
    "y_pred_dt = model_dt.predict(X_test)\n",
    "\n",
    "print_regression_metrics(y_pred_dt, \"Decision Tree\", \"Minor\")\n",
    "utils.plot_regression_results(y_test, y_pred_dt, \"Decision Tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Bagging: Random Forest & Extremly Randomized Trees\n",
    "#### Random Forest\n",
    "Implementation: [sklearn.ensemble.RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model_rf = RandomForestRegressor(n_estimators=100, oob_score=True)\n",
    "model_rf.fit(X_train, y_train)\n",
    "y_pred_rf = model_rf.predict(X_test)\n",
    "\n",
    "print_regression_metrics(y_pred_rf, \"RF\", \"Minor\")\n",
    "utils.plot_regression_results(y_test, y_pred_rf, \"RF\")\n",
    "\n",
    "print(\"Out-of-bag estimate = {0:.1f}%\".format(model_rf.oob_score_*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extremly Randomized Trees\n",
    "Implementation: [sklearn.ensemble.ExtraTreesRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html#sklearn.ensemble.ExtraTreesRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "model_et = ExtraTreesRegressor(n_estimators=100)\n",
    "model_et.fit(X_train, y_train)\n",
    "y_pred_et = model_et.predict(X_test)\n",
    "\n",
    "print_regression_metrics(y_pred_et, \"ET\", \"Minor\")\n",
    "utils.plot_regression_results(y_test, y_pred_et, \"ET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features_plotted = 15\n",
    "\n",
    "importances = model_rf.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "std = np.std([tree.feature_importances_ for tree in model_rf.estimators_], axis=0)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_train.shape[1])[125-n_features_plotted:], importances[indices][125-n_features_plotted:], color=\"r\", yerr=std[indices][125-n_features_plotted:], align=\"center\")\n",
    "# If you want to define your own labels,\n",
    "# change indices to a list of labels on the following line.\n",
    "plt.xticks(range(X_train.shape[1])[125-n_features_plotted:], indices[:n_features_plotted], rotation=90)\n",
    "plt.xlim([-1 + 125-n_features_plotted, X_train.shape[1]])\n",
    "plt.xlabel(\"Hyperspectral band\")\n",
    "plt.ylabel(\"Feature importance\")\n",
    "plt.savefig(\"plots/featureimportance_rf.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Boosting: Gradient Boosting\n",
    "Implementation: [sklearn.ensemble.GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "model_gb = GradientBoostingRegressor()\n",
    "model_gb.fit(X_train, y_train)\n",
    "y_pred_gb = model_gb.predict(X_test)\n",
    "\n",
    "print_regression_metrics(y_pred_gb, \"GB\", \"Minor\")\n",
    "utils.plot_regression_results(y_test, y_pred_gb, \"GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 3.3 Support Vector Machines\n",
    "Implementation: [sklearn.svm.SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)\n",
    "\n",
    "The SVM is tuned with a Grid Search, see [sklearn.model_selection.RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# 1. find hyperparameters\n",
    "params = {\"C\": np.logspace(-8, 8, 17), \"gamma\": np.logspace(-8, 8, 17)}\n",
    "rsearch = RandomizedSearchCV(\n",
    "    estimator=SVR(),\n",
    "    n_iter=30,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    param_distributions=params)\n",
    "rsearch.fit(X_train, y_train)\n",
    "model_svm = rsearch.best_estimator_\n",
    "\n",
    "# 2. predict\n",
    "model_svm.fit(X_train, y_train)\n",
    "y_pred_svm = model_svm.predict(X_test)\n",
    "\n",
    "print_regression_metrics(y_pred_svm, \"SVM\", \"Minor\")\n",
    "utils.plot_regression_results(y_test, y_pred_svm, \"SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 3.4 k-Nearest Neighbors\n",
    "\n",
    "Types:\n",
    "\n",
    "- [3.4.1 Without weighting](#3.4.1-Without-weighting)\n",
    "- [3.4.2 With distance weighting](#3.4.2-With-distance-weighting)\n",
    "\n",
    "Implementation: [sklearn.neighbors.KNeighborsRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor)\n",
    "\n",
    "### 3.4.1 Without weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "model_knn = KNeighborsRegressor(n_neighbors=5)\n",
    "model_knn.fit(X_train, y_train)\n",
    "y_pred_knn = model_knn.predict(X_test)\n",
    "\n",
    "print_regression_metrics(y_pred_knn, \"k-NN\", \"Minor\")\n",
    "utils.plot_regression_results(y_test, y_pred_knn, \"kNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 With distance weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "model_knnw = KNeighborsRegressor(n_neighbors=5, weights=\"distance\")\n",
    "model_knnw.fit(X_train, y_train)\n",
    "y_pred_knnw = model_knnw.predict(X_test)\n",
    "\n",
    "print_regression_metrics(y_pred_knnw, \"k-NN (weighted)\", \"Minor\")\n",
    "utils.plot_regression_results(y_test, y_pred_knnw, \"kNN weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 3.5 Artificial Neural Networks, ANN\n",
    "\n",
    "Types:\n",
    "\n",
    "- [3.5.1 Fully-connected ANNs](#3.5.1-Fully-connected-ANNs)\n",
    "- [3.5.2 CNN with Keras and TensorFlow](#3.5.2-CNN-with-Keras-and-TensorFlow)\n",
    "\n",
    "### 3.5.1 Fully-connected ANNs\n",
    "#### scikit-learn\n",
    "Implementation: [sklearn.neural_network.MLPRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "model_ann = MLPRegressor(hidden_layer_sizes=(20, 20, 20), batch_size=10, max_iter=500)\n",
    "model_ann.fit(X_train, y_train)\n",
    "y_pred_ann = model_ann.predict(X_test)\n",
    "\n",
    "print_regression_metrics(y_pred_ann, \"ANN (sklearn)\", \"Major\")\n",
    "utils.plot_regression_results(y_test, y_pred_ann, \"ANN (sklearn)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=X_train.shape[1], activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "# compile and train model\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"nadam\")\n",
    "model.fit(X_train, y_train, epochs=1000, verbose=0, batch_size=10,\n",
    "          validation_data=(X_test, y_test))\n",
    "y_pred_annk = model.predict(X_test)\n",
    "\n",
    "print_regression_metrics(y_pred_annk, \"ANN (keras)\", \"Major\")\n",
    "utils.plot_regression_results(y_test, y_pred_annk, \"ANN (keras)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 CNN with Keras and TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(filters=8, kernel_size=3, activation=\"relu\",\n",
    "                 input_shape=(X_train.shape[1],1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(filters=16, kernel_size=3, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(20, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "# compile and train model\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"nadam\")\n",
    "model.fit(X_train.reshape(X_train.shape[0], X_train.shape[1], 1), y_train,\n",
    "          epochs=500, verbose=0, batch_size=10,\n",
    "          validation_data=(X_test.reshape(X_test.shape[0], X_test.shape[1], 1), y_test))\n",
    "y_pred_cnn = model.predict(X_test.reshape(X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "print_regression_metrics(y_pred_cnn, \"CNN\", \"Major\")\n",
    "utils.plot_regression_results(y_test, y_pred_cnn, \"CNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 3.6 SUSI: Supervised Self-organizing Maps in Python\n",
    "Implementation: [felixriese/susi](https://github.com/felixriese/susi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import susi\n",
    "\n",
    "model_som = susi.SOMRegressor(\n",
    "    n_rows=35,\n",
    "    n_columns=35,\n",
    "    n_iter_unsupervised=10000,\n",
    "    n_iter_supervised=10000,\n",
    "    n_jobs=-1)\n",
    "model_som.fit(X_train, y_train)\n",
    "y_pred_som = model_som.predict(X_test)\n",
    "\n",
    "print_regression_metrics(y_pred_som, \"SOM\", \"Minor\")\n",
    "utils.plot_regression_results(y_test, y_pred_som, \"SOM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 3.7 Overall results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to CSV\n",
    "dt = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "results.to_csv(\"results/results_\"+dt+\".csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# load results from CSV\n",
    "# results = pd.read_csv(\"results/results.csv\")\n",
    "results = pd.read_csv(\"results/results_20190704-112011.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot horizontal bar plot for results\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "results.plot(x=\"model\", y=\"r2\", kind=\"barh\", ax=ax1, title=\"$R^2$\", legend=False)\n",
    "results.plot(x=\"model\", y=\"mae\", kind=\"barh\", ax=ax2, title=\"MAE\", legend=False)\n",
    "results.plot(x=\"model\", y=\"rmse\", kind=\"barh\", ax=ax3, title=\"RMSE\", legend=False)\n",
    "for ax in [ax1, ax2, ax3]:\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/results_bar.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate LaTeX table\n",
    "utils.write_results_to_latex_table(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyperspectral_regression",
   "language": "python",
   "name": "hyperspectral_regression"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
